{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cau\\miniconda3\\envs\\het\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm;\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchsummary\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import random\n",
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "CLASS_NUM = 23\n",
    "lr = 1e-4\n",
    "batch_size = 512\n",
    "epoch = 100\n",
    "patience = 100\n",
    "\n",
    "num_channels, num_rows, num_columns = 1, 40, 1292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "            \n",
    "        return torch.tensor(data, dtype=torch.float), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader, device):\n",
    "    running_correct = 0.0\n",
    "    size = len(data_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            # 답 데이터\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            running_correct += torch.sum(pred == labels)\n",
    "    running_correct = running_correct.double() / size\n",
    "    \n",
    "    return running_correct\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    train_losses = np.array(train_losses)\n",
    "    valid_losses = np.array(valid_losses)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    \n",
    "    ax.plot(train_losses, color='blue', label='Training loss')\n",
    "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "    ax.set(title='Loss over epochs',\n",
    "           xlabel='Epoch',\n",
    "           ylabel='Loss')\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    plt.style.use('default')\n",
    "\n",
    "def plot_accuracy(train_accuracy, valid_accuracy):\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    train_accuracy = np.array(train_accuracy)\n",
    "    valid_accuracy = np.array(valid_accuracy)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    \n",
    "    ax.plot(train_accuracy, color='blue', label='Training accuracy')\n",
    "    ax.plot(valid_accuracy, color='red', label='Validation accuracy')\n",
    "    ax.set(title='accuracy over epochs',\n",
    "           xlabel='Epoch',\n",
    "           ylabel='accuracy')\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss= 0.0\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # prop\n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape)\n",
    "        print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    running_correct = get_accuracy(model, train_loader, device)\n",
    "    \n",
    "    return model, optimizer, epoch_loss, running_correct.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # prop and loss\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    \n",
    "    running_correct = get_accuracy(model, valid_loader, device)\n",
    "    \n",
    "    return model, epoch_loss, running_correct.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience, verbose=False, delta=0, stop_active=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.stop_active = stop_active\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            \n",
    "            if self.stop_active == True:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                print(f'EarlyStopping counter: {self.counter}')\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), './' + model.__class__.__name__ + '_best_model.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def training_loop(model, criterion, optimizer,\n",
    "                  train_loader, valid_loader,\n",
    "                  epochs, device, early_stopping, print_every=1):\n",
    "    # best_loss = 1e10\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_acces = []\n",
    "    valid_acces = []\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        # training\n",
    "        model, optimizer, train_loss, train_acc = train(train_loader, model,\n",
    "                                             criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_acces.append(train_acc)\n",
    "        \n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss, valid_acc = validate(valid_loader, model,\n",
    "                                         criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_acces.append(valid_acc)\n",
    "            \n",
    "            early_stopping(valid_loss, model)\n",
    "        \n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "            print(\n",
    "                  f'{datetime.now().time().replace(microsecond=0)} ---'\n",
    "                  f'Epoch: {epoch + 1}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}%\\t\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}%')\n",
    "        \n",
    "        if early_stopping.early_stop == True:\n",
    "            break\n",
    "    \n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    plot_accuracy(train_acces, valid_acces)\n",
    "\n",
    "    model_state_dict = torch.load('./' + model.__class__.__name__ + '_best_model.pt', map_location=device)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    test_acc = get_accuracy(model, test_loader, device)\n",
    "    print(f'test accuracy: {100 * test_acc:.2f}%')\n",
    "\n",
    "    return model, optimizer\n",
    "    # , (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_rows * num_columns, CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class SVM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_rows * num_columns, CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=16 * 10 * 323, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=84, out_features=CLASS_NUM)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), num_channels, num_rows, num_columns)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag, scale = 'mfcc', 'test'\n",
    "features_df = pd.read_json('../Processed_Data/data_' + tag + '_' + scale + '.json')\n",
    "x = np.array(features_df.feature.tolist())\n",
    "y = np.array(features_df.class_label.tolist())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=seed)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train.reshape(x_train.shape[0], -1))\n",
    "x_train = scaler.transform(x_train.reshape(x_train.shape[0], -1))\n",
    "dump(scaler, open('../Saved_Scale/minmax_scaler.pkl', 'wb'))\n",
    "x_val = scaler.transform(x_val.reshape(x_val.shape[0], -1))\n",
    "x_test = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "train_dataset = CustomDataset(data=x_train, label=y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = CustomDataset(data=x_val, label=y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = CustomDataset(data=x_test, label=y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience, verbose=True)\n",
    "\n",
    "model = SVM().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MultiMarginLoss()\n",
    "torchsummary.summary(model, input_size=(1, 40, 1292))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vram 캐시를 없앱니다.\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model, optimizer = training_loop(model, criterion, optimizer,\n",
    "                                    train_loader, val_loader,\n",
    "                                    epoch, device, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss = nn.MultiMarginLoss()\n",
    "x = torch.tensor([[0.1, 0.2, 0.4, 0.8],\n",
    "                  [0.1, 0.2, 0.4, 0.8],\n",
    "                  [0.1, 0.2, 0.4, 0.8],\n",
    "                  [0.1, 0.2, 0.4, 0.8],\n",
    "                  [0.1, 0.2, 0.4, 0.8]\n",
    "                  ])\n",
    "print(x.shape)\n",
    "y = torch.tensor([3, 2, 1, 0, 1])\n",
    "print(y.shape)\n",
    "# 0.25 * ((1-(0.8-0.1)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))\n",
    "loss(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "het",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54e74bf99199c62ede2494f3563958cb8e322912a43717fc858ebaf84d1e028"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
