{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os\n",
    "from pickle import dump, load\n",
    "\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, AveragePooling2D, Input, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "num_rows, num_columns, num_channels = 40, 1292, 1\n",
    "CLASS_COUNT = 23\n",
    "PATIENCE = 100\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = ['2.5톤', '5톤', '9.5톤', '10톤', '27톤',\n",
    "      'K-1', 'K1a1', 'k10탄약운반차', 'K56', 'K77',\n",
    "      'K288a1', 'K200', 'K800', 'km9ace', '교량전차',\n",
    "      '다목적굴착기', '대형버스', '부식차', '살수차', '승용차',\n",
    "      '장애물개척자', '통신가설차량', '화생방정찰차'] # 중분류, 소분류\n",
    "\n",
    "def Class_ins(t):\n",
    "      if t in (5, 6):\n",
    "            return f'중분류 : 전차 소분류 : {li[t]}'\n",
    "      elif t in (7, 8, 9, 10, 11, 12, 13, 14, 20, 22):\n",
    "            return f'중분류 : 궤도/장갑차 소분류 : {li[t]}'\n",
    "      elif t in (0, 1, 2, 3, 4, 15, 16, 17, 18, 19, 21):\n",
    "            return f'중분류 : 차륜 전투차량 소분류 : {li[t]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로스, 정확도 그래프 출력\n",
    "def plot_loss_accuracy(history):\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(len(acc))\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "        plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.legend()\n",
    "        plt.figure(2)\n",
    "        plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 모델 셋팅\n",
    "def setting(model, x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS):\n",
    "        checkpoint=ModelCheckpoint(filepath='../Saved_Model/' + sys._getframe(1).f_code.co_name + '_Weight_best.hdf5', monitor=\"val_loss\",\n",
    "                                verbose=1, save_best_only=True)\n",
    "\n",
    "        ealrystopping=EarlyStopping(monitor=\"val_loss\", patience=PATIENCE)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_val, y_val),\n",
    "                        callbacks=[checkpoint, ealrystopping], verbose=1)\n",
    "        \n",
    "        plot_loss_accuracy(history)\n",
    "        \n",
    "        \n",
    "def result(x_test, y_test, model_name):\n",
    "    model = load_model('../Saved_Model/' + model_name + '_Weight_best.hdf5')\n",
    "    \n",
    "    if model_name in ('SVM', 'Logistic'):\n",
    "        x_test = x_test.reshape(x_test.shape[0], num_rows * num_columns)\n",
    "    else:\n",
    "        x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "    \n",
    "    data = {'accuracy':[], 'f1':[], 'test_time':[]}\n",
    "    \n",
    "    start = time.time()\n",
    "    data['accuracy'].append(round(model.evaluate(x_test, y_test, verbose=0)[1] * 100, 2))\n",
    "    test_time = time.time() - start\n",
    "    \n",
    "    result = model.predict(x_test)\n",
    "    \n",
    "    y_pred = np.array([np.argmax(result[i]) for i in range(result.shape[0])])\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    data['f1'].append(round(f1_score(y_test, y_pred, average='weighted') * 100, 2))\n",
    "    data['test_time'].append(round(test_time, 2))\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('../L_Result/' + model_name + '_result.csv', index=False)  # csv 파일 생성\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 정의\n",
    "def ResNet(x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS, lr):\n",
    "        EPOCHS = EPOCHS\n",
    "        BATCH_SIZE = BATCH_SIZE\n",
    "        \n",
    "        x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "        x_val = x_val.reshape(x_val.shape[0], num_rows, num_columns, num_channels)\n",
    "        \n",
    "        model = ResNet50(weights=None, input_shape=(num_rows, num_columns, num_channels), include_top=False)\n",
    "        output = model.output\n",
    "        \n",
    "        flat = Flatten()(output)\n",
    "        fc1 = Dense(1024, activation='relu')(flat)\n",
    "        fc2 = Dense(512, activation='relu')(fc1)\n",
    "        output = Dense(CLASS_COUNT, activation='softmax')(fc2)\n",
    "                \n",
    "        model = Model(inputs=model.input, outputs=output)\n",
    "        \n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"], optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "        return setting(model, x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS)\n",
    "\n",
    "\n",
    "def VGG(x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS, lr):\n",
    "        EPOCHS = EPOCHS\n",
    "        BATCH_SIZE = BATCH_SIZE\n",
    "        \n",
    "        x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "        x_val = x_val.reshape(x_val.shape[0], num_rows, num_columns, num_channels)\n",
    "        \n",
    "        model = VGG16(weights=None, input_shape=(num_rows, num_columns, num_channels), include_top=False)\n",
    "        output = model.output\n",
    "        \n",
    "        flat = Flatten()(output)\n",
    "        fc1 = Dense(4096, activation='relu')(flat)\n",
    "        fc2 = Dense(2048, activation='relu')(fc1)\n",
    "        fc3 = Dense(1024, activation='relu')(fc2)\n",
    "        output = Dense(CLASS_COUNT, activation='softmax')(fc3)\n",
    "                \n",
    "        model = Model(inputs=model.input, outputs=output)\n",
    "        \n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"], optimizer=Adam(learning_rate=lr)) # 0.0001\n",
    "\n",
    "        return setting(model, x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS)\n",
    "\n",
    "\n",
    "def LeNet(x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS, lr):\n",
    "        EPOCHS = EPOCHS\n",
    "        BATCH_SIZE = BATCH_SIZE\n",
    "        \n",
    "        x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "        x_val = x_val.reshape(x_val.shape[0], num_rows, num_columns, num_channels)\n",
    "        \n",
    "        inputs = Input(shape=(num_rows, num_columns, num_channels))\n",
    "        conv1 = Conv2D(6, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation='relu')(inputs)\n",
    "        avgpoll1 = AveragePooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(conv1)\n",
    "        conv2 = Conv2D(16, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation='relu')(avgpoll1)\n",
    "        avgpoll2 = AveragePooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(conv2)\n",
    "        flat = Flatten()(avgpoll2)\n",
    "        fc1 = Dense(120, activation='relu')(flat)\n",
    "        fc2 = Dense(84, activation='relu')(fc1)\n",
    "        outputs = Dense(CLASS_COUNT, activation='softmax')(fc2)\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        \n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"], optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "        return setting(model, x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS)\n",
    "\n",
    "\n",
    "def SVM(x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS, lr):\n",
    "        EPOCHS = EPOCHS\n",
    "        BATCH_SIZE = BATCH_SIZE\n",
    "        \n",
    "        x_train = x_train.reshape(x_train.shape[0], num_rows * num_columns)\n",
    "        x_val = x_val.reshape(x_val.shape[0], num_rows * num_columns)\n",
    "        \n",
    "        inputs = Input(shape=(num_rows * num_columns,))\n",
    "        outputs = Dense(CLASS_COUNT, activation=None)(inputs)\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        \n",
    "        model.compile(loss=\"CategoricalHinge\",\n",
    "                metrics=[\"accuracy\"], optimizer=Adam(learning_rate=lr)) # 5e-4\n",
    "\n",
    "        return setting(model, x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS)\n",
    "\n",
    "\n",
    "def Logistic(x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS, lr):\n",
    "        EPOCHS = EPOCHS\n",
    "        BATCH_SIZE = BATCH_SIZE\n",
    "        \n",
    "        x_train = x_train.reshape(x_train.shape[0], num_rows * num_columns)\n",
    "        x_val = x_val.reshape(x_val.shape[0], num_rows * num_columns)\n",
    "        \n",
    "        inputs = Input(shape=(num_rows * num_columns,))\n",
    "        outputs = Dense(CLASS_COUNT, activation='softmax')(inputs)\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        \n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"], optimizer=Adam(learning_rate=lr)) # 1e-3\n",
    "\n",
    "        return setting(model, x_train, y_train, x_val, y_val, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag, scale = 'mfcc', 'test'\n",
    "features_df = pd.read_json('../Processed_Data/data_' + tag + '_' + scale + '.json')\n",
    "\n",
    "x = np.array(features_df.feature.tolist())\n",
    "y = np.array(features_df.class_label.tolist())\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = to_categorical(le.fit_transform(y))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "scaler.fit(x_train.reshape(x_train.shape[0], -1))\n",
    "x_train = scaler.transform(x_train.reshape(x_train.shape[0], -1))\n",
    "dump(scaler, open('../Saved_Scale/minmax_scaler.pkl', 'wb'))\n",
    "x_val = scaler.transform(x_val.reshape(x_val.shape[0], -1))\n",
    "x_test = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns)\n",
    "x_val = x_val.reshape(x_val.shape[0], num_rows, num_columns)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = load(open('../Saved_Scale/minmax_scaler.pkl', 'rb'))\n",
    "# Logistic(x_train, y_train, x_val, y_val, BATCH_SIZE=512, EPOCHS=300, lr=1e-4)\n",
    "# result(x_test, y_test, 'Logistic')\n",
    "\n",
    "SVM(x_train, y_train, x_val, y_val, BATCH_SIZE=512, EPOCHS=100, lr=5e-4)\n",
    "result(x_test, y_test, 'SVM')\n",
    "\n",
    "# LeNet(x_train, y_train, x_val, y_val, BATCH_SIZE=512, EPOCHS=300, lr=1e-3)\n",
    "# result(x_test, y_test, 'LeNet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "het",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54e74bf99199c62ede2494f3563958cb8e322912a43717fc858ebaf84d1e028"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
